{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b80e43b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Splinter and BeautifulSoup\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Import pandas for .read_html() function\n",
    "import pandas as pd\n",
    "\n",
    "# add import datetime as dt\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88d5053f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 92.0.4515\n",
      "Get LATEST driver version for 92.0.4515\n",
      "Driver [C:\\Users\\Gary Laptop\\.wdm\\drivers\\chromedriver\\win32\\92.0.4515.107\\chromedriver.exe] found in cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'news_title': \"The Launch Is Approaching for NASA's Next Mars Rover, Perseverance\", 'news_paragraph': \"The Red Planet's surface has been visited by eight NASA spacecraft. The ninth will be the first that includes a roundtrip ticket in its flight plan. \", 'featured_image': 'https://data-class-jpl-space.s3.amazonaws.com/JPL_Space/image/featured/mars1.jpg', 'facts': '<table border=\"1\" class=\"dataframe table table-striped\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th></th>\\n      <th>Mars</th>\\n      <th>Earth</th>\\n    </tr>\\n    <tr>\\n      <th>Description</th>\\n      <th></th>\\n      <th></th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <th>Mars - Earth Comparison</th>\\n      <td>Mars</td>\\n      <td>Earth</td>\\n    </tr>\\n    <tr>\\n      <th>Diameter:</th>\\n      <td>6,779 km</td>\\n      <td>12,742 km</td>\\n    </tr>\\n    <tr>\\n      <th>Mass:</th>\\n      <td>6.39 × 10^23 kg</td>\\n      <td>5.97 × 10^24 kg</td>\\n    </tr>\\n    <tr>\\n      <th>Moons:</th>\\n      <td>2</td>\\n      <td>1</td>\\n    </tr>\\n    <tr>\\n      <th>Distance from Sun:</th>\\n      <td>227,943,824 km</td>\\n      <td>149,598,262 km</td>\\n    </tr>\\n    <tr>\\n      <th>Length of Year:</th>\\n      <td>687 Earth days</td>\\n      <td>365.24 days</td>\\n    </tr>\\n    <tr>\\n      <th>Temperature:</th>\\n      <td>-87 to -5 °C</td>\\n      <td>-88 to 58°C</td>\\n    </tr>\\n  </tbody>\\n</table>', 'last_modified': datetime.datetime(2021, 8, 21, 16, 23, 21, 638988)}\n"
     ]
    }
   ],
   "source": [
    "def scrape_all():\n",
    "    # Initiate headless driver for deployment\n",
    "    executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "    browser = Browser('chrome', **executable_path, headless=True)\n",
    "\n",
    "    news_title, news_paragraph = mars_news(browser)\n",
    "\n",
    "    # Run all scraping functions and store results in a dictionary\n",
    "    data = {\n",
    "        \"news_title\": news_title,\n",
    "        \"news_paragraph\": news_paragraph,\n",
    "        \"featured_image\": featured_image(browser),\n",
    "        \"facts\": mars_facts(),\n",
    "        \"last_modified\": dt.datetime.now()\n",
    "    }\n",
    "\n",
    "    # Stop webdriver and return data\n",
    "    browser.quit()\n",
    "    return data\n",
    "\n",
    "\n",
    "def mars_news(browser):\n",
    "\n",
    "    # Scrape Mars News\n",
    "    # Visit the mars nasa news site\n",
    "    url = 'https://data-class-mars.s3.amazonaws.com/Mars/index.html'\n",
    "    browser.visit(url)\n",
    "\n",
    "    # Optional delay for loading the page\n",
    "    browser.is_element_present_by_css('div.list_text', wait_time=1)\n",
    "\n",
    "    # Convert the browser html to a soup object and then quit the browser\n",
    "    html = browser.html\n",
    "    news_soup = soup(html, 'html.parser')\n",
    "\n",
    "    # Add try/except for error handling\n",
    "    try:\n",
    "        slide_elem = news_soup.select_one('div.list_text')\n",
    "        # Use the parent element to find the first 'a' tag and save it as 'news_title'\n",
    "        news_title = slide_elem.find('div', class_='content_title').get_text()\n",
    "        # Use the parent element to find the paragraph text\n",
    "        news_p = slide_elem.find('div', class_='article_teaser_body').get_text()\n",
    "\n",
    "    except AttributeError:\n",
    "        return None, None\n",
    "\n",
    "    return news_title, news_p\n",
    "\n",
    "\n",
    "def featured_image(browser):\n",
    "    # Visit URL\n",
    "    url = 'https://data-class-jpl-space.s3.amazonaws.com/JPL_Space/index.html'\n",
    "    browser.visit(url)\n",
    "\n",
    "    # Find and click the full image button\n",
    "    full_image_elem = browser.find_by_tag('button')[1]\n",
    "    full_image_elem.click()\n",
    "\n",
    "    # Parse the resulting html with soup\n",
    "    html = browser.html\n",
    "    img_soup = soup(html, 'html.parser')\n",
    "\n",
    "    # Add try/except for error handling\n",
    "    try:\n",
    "        # Find the relative image url\n",
    "        img_url_rel = img_soup.find('img', class_='fancybox-image').get('src')\n",
    "\n",
    "    except AttributeError:\n",
    "        return None\n",
    "\n",
    "    # Use the base url to create an absolute url\n",
    "    img_url = f'https://data-class-jpl-space.s3.amazonaws.com/JPL_Space/{img_url_rel}'\n",
    "\n",
    "    return img_url\n",
    "\n",
    "def mars_facts():\n",
    "    # Add try/except for error handling\n",
    "    try:\n",
    "        # Use 'read_html' to scrape the facts table into a dataframe\n",
    "        df = pd.read_html('https://data-class-mars-facts.s3.amazonaws.com/Mars_Facts/index.html')[0]\n",
    "\n",
    "    except BaseException:\n",
    "        return None\n",
    "\n",
    "    # Assign columns and set index of dataframe\n",
    "    df.columns=['Description', 'Mars', 'Earth']\n",
    "    df.set_index('Description', inplace=True)\n",
    "\n",
    "    # Convert dataframe into HTML format, add bootstrap\n",
    "    return df.to_html(classes=\"table table-striped\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # If running as script, print scraped data\n",
    "    print(scrape_all())\n",
    "    \n",
    "# I copied the above code from what was supposed to go on scrapping.py.  The code in the notes below is probably more \n",
    "# reliable to go by for the challenge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65e2f013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set your executable path then set up the URL\n",
    "#executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "#browser = Browser('chrome', **executable_path, headless=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87f59893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the url and instruct the browser to visit it\n",
    "# Visit the mars nasa news site\n",
    "#url = 'https://redplanetscience.com'\n",
    "#browser.visit(url)\n",
    "# Optional delay for loading the page\n",
    "#browser.is_element_present_by_css('div.list_text', wait_time=1)\n",
    "\n",
    "# With the following line, browser.is_element_present_by_css('div.list_text', wait_time=1), we are \n",
    "# accomplishing two things.\n",
    "\n",
    "# One is that we're searching for elements with a specific combination of tag (div) and attribute (list_text). As an\n",
    "# example, ul.item_list would be found in HTML as <ul class=\"item_list\">.\n",
    "\n",
    "# Secondly, we're also telling our browser to wait one second before searching for components. \n",
    "# The optional delay is useful because sometimes dynamic pages take a little while to load, especially if they are\n",
    "# image-heavy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54cad57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the HTML parser:\n",
    "#html = browser.html\n",
    "#news_soup = soup(html, 'html.parser')\n",
    "#slide_elem = news_soup.select_one('div.list_text')\n",
    "\n",
    "# Notice how we've assigned slide_elem as the variable to look for the <div /> tag and its descendent \n",
    "# (the other tags within the <div /> element)? This is our parent element. This means that this element holds all of\n",
    "# the other elements within it, and we'll reference it when we want to filter search results even further. The . is used \n",
    "# for selecting classes, such as list_text, so the code 'div.list_text' pinpoints the <div /> tag with the\n",
    "# class of list_text. CSS works from right to left, such as returning the last item on the list instead of the first. \n",
    "# Because of this, when using select_one, the first matching element returned will be a <li /> element with a class of\n",
    "# slide and all nested elements within it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfbe24d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After opening the page in a new browser, right-click to inspect and activate your DevTools. Then search for\n",
    "# the HTML components you'll use to identify the title and paragraph you want\n",
    "# What we will search is: class = “content_title”\n",
    "\n",
    "# We'll want to assign the title and summary text to variables we'll reference later\n",
    "# begin our scraping:\n",
    "#slide_elem.find('div', class_='content_title')\n",
    "\n",
    "# In this line of code, we chained .find onto our previously assigned variable, slide_elem. When we do this,\n",
    "# we're saying, \"This variable holds a ton of information, so look inside of that information to find this specific\n",
    "# data.\" The data we're looking for is the content title, which we've specified by saying, \"The specific data is in \n",
    "# a <div /> with a class of 'content_title'.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7485dedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The title is in that mix of HTML in our output—that's awesome! But we need to get just the text, and the extra\n",
    "# HTML stuff isn't necessary. \n",
    "\n",
    "# Use the parent element to find the first `a` tag and save it as `news_title`\n",
    "#news_title = slide_elem.find('div', class_='content_title').get_text()\n",
    "#news_title\n",
    "\n",
    "# We've added something new to our .find() method here: .get_text(). When this new method is chained onto .find(), only \n",
    "# the text of the element is returned. The code above, for example, would return only the title of the news article and not\n",
    "# any of the HTML tags or elements\n",
    "\n",
    "# Once executed, the result is the most recent title published on the website. When the website is updated and a new \n",
    "# article is posted, when our code is run again, it will return that article instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32a08981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we need to add the summary text.\n",
    "# Use the DevTools selector tool and select the article summary (teaser), then check to see which tag is highlighted.\n",
    "\n",
    "# We know that \"article_teaser_body\" is the right class name, but when we search for it, there is more than one \n",
    "# result. What now?\n",
    "\n",
    "# That's okay. There will be many matches because there are many articles, each with a tag of <div /> and a class of\n",
    "# article_teaser_body. We want to pull the first one on the list, not a specific one, so more than 10 results is fine. \n",
    "# In this case, if our scraping code is too specific, we'd pull only that article summary instead of the most recent.\n",
    "\n",
    "# Because new articles are added to the top of the list, and we only need the most recent one, our search leads us to the\n",
    "# first article.\n",
    "\n",
    "# There are two methods used to find tags and attributes with BeautifulSoup:\n",
    "\n",
    "# .find() is used when we want only the first class and attribute we've specified.\n",
    "# .find_all() is used when we want to retrieve all of the tags and attributes.\n",
    "# For example, if we were to use .find_all() instead of .find() when pulling the summary, we would retrieve all of\n",
    "# the summaries on the page instead of just the first one.\n",
    "\n",
    "# Use the parent element to find the paragraph text\n",
    "#news_p = slide_elem.find('div', class_='article_teaser_body').get_text()\n",
    "#news_p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04579e7c",
   "metadata": {},
   "source": [
    "# Our next step scraping code will be to gather the featured images from the Jet Propulsion Laboratory's Space \n",
    "# Images (Links to an external site.) webpage. In your Jupyter notebook, use markdown to separate the article scraping \n",
    "# from the image scraping.\n",
    "\n",
    "### Featured Images\n",
    "\n",
    "# change the format of the code cell to \"Markdown.\"\n",
    "# You can access the cell formatting feature by using a drop-down menu at the top of the notebook. It's currently \n",
    "# set to \"Code,\" so click the down arrow to toggle the drop-down menu and select \"Markdown\" instead.\n",
    "# This would normally just be a cell that says featured images and is changed to markdown, but I have these notes to remember it. No code should go in this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c0149be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visit URL\n",
    "#url = 'https://spaceimages-mars.com'\n",
    "#browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "249987c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we want to click the \"Full Image\" button. This button will direct our browser to an image slideshow.\n",
    "# Let's take a look at the button's HTML tags and attributes with the DevTools\n",
    "# <button class=\"btn btn-outline-light\"> FULL IMAGE</button>\n",
    "# This is a fairly straightforward HTML tag: the <button> element has a two \n",
    "# classes (btn and btn-outline-light) and a string reading \"FULL IMAGE\".\n",
    "\n",
    "# First, let's use the dev tools to search for all the button elements. There are 3 of them.\n",
    "# Since there are only three buttons, and we want to click the full-size image button, we can go ahead and use\n",
    "# the HTML tag in our code.\n",
    "\n",
    "# Find and click the full image button\n",
    "#full_image_elem = browser.find_by_tag('button')[1]\n",
    "#full_image_elem.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c507abc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the new page loaded onto our automated browser, it needs to be parsed so we can continue and scrape t\n",
    "# he full-size image URL\n",
    "\n",
    "# Parse the resulting html with soup\n",
    "#html = browser.html\n",
    "#img_soup = soup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e7eb0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to find the relative image URL. In our browser (make sure you're on the same page as the automated one),\n",
    "# activate your DevTools again. This time, let's find the image link for that image. \n",
    "\n",
    "# We want to pull the most recently posted image for our web app\n",
    "# It's important to note that the value of the src will be different every time the page is updated, so we\n",
    "# can't simply record the current value—we would only pull that image each time the code is executed, instead of the most\n",
    "# recent one.\n",
    "\n",
    "# We'll use the image tag and class (<img />and fancybox-img) to build the URL to the full-size image. \n",
    "\n",
    "# Find the relative image url\n",
    "#img_url_rel = img_soup.find('img', class_='fancybox-image').get('src')\n",
    "#img_url_rel\n",
    "\n",
    "# We've done a lot with that single line.\n",
    "    # An img tag is nested within this HTML, so we've included it.\n",
    "    # .get('src') pulls the link to the image.\n",
    "    \n",
    "# What we've done here is tell BeautifulSoup to look inside the <img /> tag for an image with a class of fancybox-image. \n",
    "# Basically we're saying, \"This is where the image we want lives—use the link that's inside these tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f087834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we copy and paste this link into a browser, it won't work. This is because it's only a partial link, as the base\n",
    "# URL isn't included.\n",
    "\n",
    "# Let's add the base URL to our code.\n",
    "# Use the base URL to create an absolute URL\n",
    "#img_url = f'https://spaceimages-mars.com/{img_url_rel}'\n",
    "#img_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a9c523f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We've chosen to collect our data from Mars Facts (Links to an external site.), so let's visit the webpage to look at\n",
    "# what we'll be working with. We already have a great photo and an article, so all we want from this page is the\n",
    "# table. Our plan is to display it as a table on our own web app, so keeping the current HTML table format is important.\n",
    "\n",
    "# Let's look at the webpage again, this time using our DevTools. All of the data we want is in a <table /> tag. HTML\n",
    "# code used to create a table looks fairly complex, but it's really just breaking down and naming each component.\n",
    "\n",
    "# Tables in HTML are basically made up of many smaller containers. The main container is the <table /> tag. Inside \n",
    "# the table is <tbody />, which is the body of the table—the headers, columns, and rows.\n",
    "\n",
    "# <tr /> is the tag for each table row. Within that tag, the table data is stored in <td /> tags. This is where\n",
    "# the columns are established.\n",
    "\n",
    "# Instead of scraping each row, or the data in each <td />, we're going to scrape the entire table \n",
    "# with Pandas' .read_html() function.\n",
    "\n",
    "# At the top of your Jupyter Notebook, add import pandas as pd to the dependencies and rerun the cell. This \n",
    "# way, we'll be able to use this new function without generating an error\n",
    "\n",
    "# Turn the table into a dataframe basically\n",
    "#df = pd.read_html('https://galaxyfacts-mars.com')[0]\n",
    "#df.columns=['description', 'Mars', 'Earth']\n",
    "#df.set_index('description', inplace=True)\n",
    "#df\n",
    "\n",
    "# df = pd.read_htmldf = pd.read_html('https://galaxyfacts-mars.com')[0] With this line, we're creating a new DataFrame \n",
    "# from the HTML table. The Pandas function read_html() specifically searches for and returns a list of tables found in the\n",
    "# HTML. By specifying an index of 0, we're telling Pandas to pull only the first table it encounters, or the first item \n",
    "# in the list. Then, it turns the table into a DataFrame.\n",
    "\n",
    "# df.columns=['description', 'Mars', 'Earth'] Here, we assign columns to the new DataFrame for additional clarity.\n",
    "\n",
    "# df.set_index('description', inplace=True) By using the .set_index() function, we're turning the Description column\n",
    "# into the DataFrame's index. inplace=True means that the updated index will remain in place, without having to reassign \n",
    "# the DataFrame to a new variable.\n",
    "\n",
    "# Now, when we call the DataFrame, we're presented with a tidy, Pandas-friendly representation of the HTML table we \n",
    "# were just viewing on the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b1681ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How do we add the DataFrame to a web application? Our data is live—if the table is updated, then we want\n",
    "# that change to appear in our app\n",
    "\n",
    "# Thankfully, Pandas also has a way to easily convert our DataFrame back into HTML-ready code using\n",
    "# the .to_html() function:\n",
    "\n",
    "#df.to_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abe9ec37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# End the session\n",
    "#browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1196d6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Export to Python\n",
    "\n",
    "# we can't automate the scraping using the Jupyter Notebook. To fully automate it, it will need to be \n",
    "# converted into a .py file.\n",
    "\n",
    "# The next step in making this an automated process is to download the current code into a Python file. It won't \n",
    "# transition over perfectly, we'll need to clean it up a bit, but it's an easier task than copying each \n",
    "# cell and pasting it over in the correct order\n",
    "\n",
    "# The Jupyter ecosystem is an extremely versatile tool. We already know many of its great functions, such as the\n",
    "# different libraries that work well with it and also how easy it is to troubleshoot code. Another feature is being \n",
    "# able to download the notebook into different formats.\n",
    "\n",
    "# There are several formats available, but we'll focus on one by downloading to a Python file.\n",
    "\n",
    "    # 1. While your notebook is open, navigate to the top of the page to the Files tab.\n",
    "    \n",
    "    # 2. From here, scroll down to the \"Download as\" section of the drop-down menu.\n",
    "    \n",
    "    # 3. Select \"Python (.py)\" from the next menu to download the code\n",
    "    \n",
    "    # 4. If you get a warning about downloading this type of file, click \"Keep\" to continue the download.\n",
    "    \n",
    "    # 5. Navigate to your Downloads folder and open the new file. A brief look at the first lines of code shows us\n",
    "    # that the code wasn't the only thing to be ported over. The number of times each cell has been run is also there, \n",
    "    # for example\n",
    "    \n",
    "    # 6. Clean up the code by removing unnecessary blank spaces and comments. When you're done tidying up the code,\n",
    "    # make sure you save it in your working folder with your notebook code as scraping.py. You can also test the script by\n",
    "    # running it through your terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "435affe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want our code to be reused, and often, to pull the most recent data.\n",
    "# Functions enable this capability by bundling our code into something that is easy for us (and once it's deployed, \n",
    "# whoever else we share the web app with) to use and reuse as needed.\n",
    "\n",
    "# Each major scrape, such as the news title and paragraph or featured image, will be divided into a \n",
    "# self-contained, reusable function.\n",
    "\n",
    "### News Title and Paragraph\n",
    "# we will revisit the news title and paragraph code above and insert it into a function. Let's call it mars_news\n",
    "# Begin the function by defining it, then indent the code as needed to adhere to function syntax\n",
    "\n",
    "#def mars_news():\n",
    "\n",
    "   # Visit the mars nasa news site\n",
    "   #url = 'https://redplanetscience.com/'\n",
    "   #browser.visit(url)\n",
    "\n",
    "   # Optional delay for loading the page\n",
    "   #browser.is_element_present_by_css('div.list_text', wait_time=1)\n",
    "\n",
    "   # Convert the browser html to a soup object and then quit the browser\n",
    "   #html = browser.html\n",
    "   #news_soup = soup(html, 'html.parser')\n",
    "\n",
    "   #slide_elem = news_soup.select_one('div.list_text')\n",
    "   #slide_elem.find('div', class_='content_title')\n",
    "\n",
    "   # Use the parent element to find the first <a> tag and save it as  `news_title`\n",
    "   #news_title = slide_elem.find('div', class_='content_title').get_text()\n",
    "   #news_title\n",
    "\n",
    "   # Use the parent element to find the paragraph text\n",
    "   #news_p = slide_elem.find('div', class_='article_teaser_body').get_text()\n",
    "   #news_p\n",
    "    \n",
    "   #return mars_news()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45fc76d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of having our title and paragraph printed within the function, we want to return them from the function\n",
    "# so they can be used outside of it. We'll adjust our code to do so by deleting news_title and news_p and include\n",
    "# them in the return statement instead\n",
    "\n",
    "#def mars_news():\n",
    "\n",
    "   # Visit the mars nasa news site\n",
    "   #url = 'https://redplanetscience.com/'\n",
    "   #browser.visit(url)\n",
    "\n",
    "   # Optional delay for loading the page\n",
    "   #browser.is_element_present_by_css('div.list_text', wait_time=1)\n",
    "\n",
    "   # Convert the browser html to a soup object and then quit the browser\n",
    "   #html = browser.html\n",
    "   #news_soup = soup(html, 'html.parser')\n",
    "\n",
    "   #slide_elem = news_soup.select_one('div.list_text')\n",
    "\n",
    "   # Use the parent element to find the first <a> tag and save it as `news_title`\n",
    "   #news_title = slide_elem.find('div', class_='content_title').get_text()\n",
    "\n",
    "   # Use the parent element to find the paragraph text\n",
    "   #news_p = slide_elem.find('div', class_='article_teaser_body').get_text()\n",
    "\n",
    "   #return news_title, news_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5827f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are two things left to do. First, we need to add an argument to the function.\n",
    "# The finishing touch is to add error handling to the mix. This is to address any potential errors that may\n",
    "# occur during web scraping. We're going to add a try and except clause addressing AttributeErrors. By adding this error\n",
    "# handling, we are able to continue with our other scraping portions even if this one doesn't work\n",
    "# After adding the try portion of our error handling, we need to add the except part. By adding try: just before \n",
    "# scraping, we're telling Python to look for these elements. If there's an error, Python will continue to run the remainder\n",
    "# of the code. If it runs into an AttributeError, however, instead of returning the title and paragraph, Python will\n",
    "# return nothing instead.\n",
    "\n",
    "#def mars_news(browser):\n",
    "\n",
    "    # Scrape Mars News\n",
    "    # Visit the mars nasa news site\n",
    "    #url = 'https://redplanetscience.com/'\n",
    "    #browser.visit(url)\n",
    "\n",
    "    # Optional delay for loading the page\n",
    "    #browser.is_element_present_by_css('div.list_text', wait_time=1)\n",
    "\n",
    "    # Convert the browser html to a soup object and then quit the browser\n",
    "    #html = browser.html\n",
    "    #news_soup = soup(html, 'html.parser')\n",
    "\n",
    "    # Add try/except for error handling\n",
    "    #try:\n",
    "        #slide_elem = news_soup.select_one('div.list_text')\n",
    "        # Use the parent element to find the first 'a' tag and save it as 'news_title'\n",
    "        #news_title = slide_elem.find('div', class_='content_title').get_text()\n",
    "        # Use the parent element to find the paragraph text\n",
    "        #news_p = slide_elem.find('div', class_='article_teaser_body').get_text()\n",
    "\n",
    "    #except AttributeError:\n",
    "        #return None, None\n",
    "\n",
    "    #return news_title, news_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc9a4329",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Featured Image\n",
    "\n",
    "# Declare and define our function.\n",
    "\n",
    "#def featured_image(browser):\n",
    "# Remove print statement(s) and return them instead.\n",
    "\n",
    "# In our Jupyter Notebook version of the code, we printed the results of our scraping by simply stating the \n",
    "# variable (e.g., after assigning data to the img_url variable, we simply put img_url on the next line to view the data).\n",
    "# We still want to view the data output in our Python script, but we want to see it at the end of our function instead of \n",
    "# within it.\n",
    "\n",
    "#return img_url\n",
    "# Add error handling for AttributeError.\n",
    "\n",
    "#try:\n",
    "   # find the relative image url\n",
    "   #img_url_rel = img_soup.find('img', class_='fancybox-image').get('src')\n",
    "\n",
    "#except AttributeError:\n",
    "   #return None\n",
    "# All together, this function should look as follows:\n",
    "\n",
    "#def featured_image(browser):\n",
    "    # Visit URL\n",
    "    #url = 'https://spaceimages-mars.com'\n",
    "    #browser.visit(url)\n",
    "\n",
    "    # Find and click the full image button\n",
    "    #full_image_elem = browser.find_by_tag('button')[1]\n",
    "    #full_image_elem.click()\n",
    "\n",
    "    # Parse the resulting html with soup\n",
    "    #html = browser.html\n",
    "    #img_soup = soup(html, 'html.parser')\n",
    "\n",
    "    # Add try/except for error handling\n",
    "    #try:\n",
    "        # Find the relative image url\n",
    "        #img_url_rel = img_soup.find('img', class_='fancybox-image').get('src')\n",
    "\n",
    "    #except AttributeError:\n",
    "        #return None\n",
    "\n",
    "    # Use the base url to create an absolute url\n",
    "    #img_url = f'https://spaceimages-mars.com/{img_url_rel}'\n",
    "\n",
    "    #return img_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57b90ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Mars Facts\n",
    "\n",
    "# A BaseException is a little bit of a catchall when it comes to error handling. It is raised when any of the built-in\n",
    "# exceptions are encountered and it won't handle any user-defined exceptions. We're using it here because we're\n",
    "# using Pandas' read_html() function to pull data, instead of scraping with BeautifulSoup and Splinter. The data is \n",
    "# returned a little differently and can result in errors other than AttributeErrors, which is what we've been addressing \n",
    "# so far.\n",
    "\n",
    "# Let's first define our function:\n",
    "\n",
    "#def mars_facts():\n",
    "# Next, we'll update our code by adding the try and except block.\n",
    "\n",
    "   #try:\n",
    "      # use 'read_html\" to scrape the facts table into a dataframe\n",
    "      #df = pd.read_html('https://galaxyfacts-mars.com')[0]\n",
    "   #except BaseException:\n",
    "      #return None\n",
    "# As before, we've removed the print statements. Now that we know this code is working correctly, we don't need to \n",
    "# view the DataFrame that's generated.\n",
    "\n",
    "# The code to assign columns and set the index of the DataFrame will remain the same, so the last update we need to\n",
    "# complete for this function is to add the return statement.\n",
    "\n",
    "   #return df.to_html()\n",
    "# The full mars_facts function should look like this:\n",
    "\n",
    "#def mars_facts():\n",
    "    # Add try/except for error handling\n",
    "    #try:\n",
    "        # Use 'read_html' to scrape the facts table into a dataframe\n",
    "        #df = pd.read_html('https://galaxyfacts-mars.com')[0]\n",
    "\n",
    "    #except BaseException:\n",
    "        #return None\n",
    "\n",
    "    # Assign columns and set index of dataframe\n",
    "    #df.columns=['Description', 'Mars', 'Earth']\n",
    "    #df.set_index('Description', inplace=True)\n",
    "\n",
    "    # Convert dataframe into HTML format, add bootstrap\n",
    "    #return df.to_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "85b14a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we're ready to integrate Mongo Into the Web App\n",
    "# The next step is to integrate Mongo into the web app. We want the script to update the data stored in Mongo each time \n",
    "# it's run. We need to add just a little bit more code to our scraping.py script to establish the link between scraped data\n",
    "# and the database.\n",
    "\n",
    "# At the top of our scraping.py script, just after importing the dependencies, we'll add one more function. This function differs from the others in that it will:\n",
    "\n",
    "# Initialize the browser.\n",
    "# Create a data dictionary.\n",
    "# End the WebDriver and return the scraped data.\n",
    "# Let's define this function as \"scrape_all\" and then initiate the browser.\n",
    "\n",
    "#def scrape_all():\n",
    "    # Initiate headless driver for deployment\n",
    "    #executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "    #browser = Browser('chrome', **executable_path, headless=True)\n",
    "    \n",
    "    # This was put in the second cell after the dependencies. The following cells are just notes on this process and \n",
    "    # the rest of the code we're adding to that celll. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f3f61b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we're going to set our news title and paragraph variables (remember, this function will return two values).\n",
    "# This line of code tells Python that we'll be using our mars_news function to pull this data.\n",
    "\n",
    "#news_title, news_paragraph = mars_news(browser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "342c533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have our browser ready for work, we need to create the data dictionary. Add the following code to\n",
    "# our scrape_all() function:\n",
    "\n",
    "# Run all scraping functions and store results in dictionary\n",
    "#data = {\n",
    "      #\"news_title\": news_title,\n",
    "      #\"news_paragraph\": news_paragraph,\n",
    "      #\"featured_image\": featured_image(browser),\n",
    "      #\"facts\": mars_facts(),\n",
    "      #\"last_modified\": dt.datetime.now()\n",
    "#}\n",
    "\n",
    "# This dictionary does two things: It runs all of the functions we've created—featured_image(browser), for example—and\n",
    "# it also stores all of the results. When we create the HTML template, we'll create paths to the dictionary's values, \n",
    "# which lets us present our data on our template. We're also adding the date the code was run last by \n",
    "# adding \"last_modified\": dt.datetime.now(). For this line to work correctly, we'll also need to add import datetime as\n",
    "# dt to our imported dependencies at the beginning of our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f873e0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To finish up the function, there are two more things to do. The first is to end the WebDriver using the line\n",
    "# browser.quit(). You can quit the automated browser by physically closing it, but there's a chance it won't fully quit\n",
    "# in the background. By using code to exit the browser, you'll know that all of the processes have been stopped.\n",
    "\n",
    "# Second, the return statement needs to be added. This is the final line that will signal that the function is complete, \n",
    "# and it will be inserted directly beneath browser.quit(). We want to return the data dictionary created earlier, so \n",
    "#our return statement will simply read return data.\n",
    "\n",
    "# Stop webdriver and return data\n",
    "#browser.quit()\n",
    "#return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5cf35bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The last step we need to add is similar to the last code block in our app.py file.\n",
    "#if __name__ == \"__main__\":\n",
    "    # If running as script, print scraped data\n",
    "    #print(scrape_all())\n",
    "\n",
    "    ### The last few cells are in the second cell. Thes are just the notes for it. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
